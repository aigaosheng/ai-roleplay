{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b195aa9-dd04-42dd-ade9-b3a29d710bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from typing import List\n",
    "# from langchain.chat_models import ChatVertexAI\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    \n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "model_name = \"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\"\n",
    "model_name_embed = \"/home/gs/hf_home/models/models--google--gemma-2b/gemma-2b.gguf\"\n",
    "\n",
    "#define consistent parametes\n",
    "# n_batch >= chunk-size\n",
    "chunk_size = 512\n",
    "\n",
    "# lm_embed_model = LlamaCppEmbeddings(model_path = model_name, n_gpu_layers = -1, n_ctx = 512 * 4, n_batch = chunk_size, verbose=True)\n",
    "\n",
    "llm_chat_model = LlamaCpp(\n",
    "        model_path=model_name,\n",
    "        n_gpu_layers=-1,\n",
    "        # n_batch = chunk_size,\n",
    "        # callback_manager=callback_manager,\n",
    "        n_ctx=1024*2, # Uncomment to increase the context window\n",
    "        # temperature=0.75,\n",
    "        # f16_kv=True,\n",
    "        verbose=False,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6addf4d-5c7b-485c-9e56-c2206d80b490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n\\n**Challenge:**\\n\\nWrite a pitch for AIA insurance product that focuses on the benefits for a young professional looking to buy their first home. \\n\\n**Answer:**\\n\\nHey there! I'm your AIA Insurance agent here to help you secure your first home with our exclusive first-time buyer program!\\n\\n**Benefits:**\\n\\n* **Low down payment:** With our competitive rates, you can get started with just 20% down payment, making homeownership more attainable than ever.\\n* **Flexible loan terms:** We offer various loan terms and repayment options that fit your budget and financial situation.\\n* **Low interest rates:** Our low interest rates can save you thousands of dollars over the life of your loan.\\n* **Homeowners insurance included:** Enjoy comprehensive homeowners insurance included in your loan package, giving you peace of mind and protection against unexpected events.\\n* **Credit building opportunities:** Our program offers opportunities to build your credit, which can make it easier to qualify for future mortgages or loans.\\n* **No upfront costs:** There are no hidden charges or fees, ensuring a transparent and straightforward loan process.\\n\\n**Call to action:**\\n\\nDon't miss out on this incredible opportunity to secure your dream home with AIA Insurance's first-time\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I am the AIA insurance compancy agent to sell the insurance product. You are my customer too buy insurance product from me. You help me improve pitch skill. You challenging me, and I answer.\n",
    "\"\"\"\n",
    "\n",
    "llm_chat_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01bd210-b197-4b2f-9cd2-6ebc7fe68689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gemma import GemmaLocal, GemmaChatLocal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cef9e2-4ab4-4b82-9b53-1b54b646ceed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    36.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   36.00 MiB, K (f16):   18.00 MiB, V (f16):   18.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     7.88 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n"
     ]
    }
   ],
   "source": [
    "from gemma import GemmaLocal, GemmaChatLocal\n",
    "# from langchain_google_vertexai import GemmaChatLocalHF, GemmaLocalHF\n",
    "\n",
    "ChatOpenAI = GemmaChatLocal(model_name = model_name, hf_access_token = \"\")\n",
    "\n",
    "\n",
    "class CAMELAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_message: SystemMessage,\n",
    "        model: GemmaChatLocal,\n",
    "    ) -> None:\n",
    "        self.system_message = system_message\n",
    "        self.model = model\n",
    "        self.init_messages()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.init_messages()\n",
    "        return self.stored_messages\n",
    "\n",
    "    def init_messages(self) -> None:\n",
    "        self.stored_messages = [self.system_message]\n",
    "\n",
    "    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n",
    "        self.stored_messages.append(message)\n",
    "        return self.stored_messages\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        input_message: HumanMessage,\n",
    "    ) -> AIMessage:\n",
    "        messages = self.update_messages(input_message)\n",
    "\n",
    "        output_message = self.model(messages)\n",
    "        self.update_messages(output_message)\n",
    "\n",
    "        return output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9cd7f3-a7e7-42b0-84ac-3fdaa718adf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =     231.44 ms\n",
      "llama_print_timings:      sample time =     240.09 ms /   256 runs   (    0.94 ms per token,  1066.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     463.14 ms /    16 tokens (   28.95 ms per token,    34.55 tokens per second)\n",
      "llama_print_timings:        eval time =   34674.36 ms /   255 runs   (  135.98 ms per token,     7.35 tokens per second)\n",
      "llama_print_timings:       total time =   37899.78 ms /   271 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**The Meaning of Life**\\n\\n**Philosophical Perspectives:**\\n\\n* **Existentialism:** Life is inherently meaningless and without intrinsic purpose. Existentialists believe that we create our own meaning through our actions and choices.\\n* **Nihilism:** Life is devoid of any inherent meaning or value. Nihilists reject the possibility of any objective truth or purpose in existence.\\n* **Stoicism:** Life is a random and indifferent universe, with no inherent meaning or purpose. Stoics focus on living in the present moment and accepting what is beyond our control.\\n* **Religion:** Many religions provide a moral framework that outlines the meaning of life, typically based on a divine creator or purpose.\\n\\n**Personal Perspectives:**\\n\\n* **Self-Actualization:** Some individuals strive for personal growth, self-fulfillment, and reaching their full potential.\\n* **Connection:** Others find meaning in relationships, family, and contributing to a greater good.\\n* **Service to Others:** Helping others and making a positive impact on the world can be a fulfilling way to contribute to meaning.\\n* **Experiencing Beauty and Wonder:** Exploring the natural world, pursuing knowledge, and engaging in activities that evoke a sense of awe can bring meaning to life.\\n* **Leaving'\n"
     ]
    }
   ],
   "source": [
    "output = ChatOpenAI.invoke([\"What is the meaning of life?\"], max_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6e0029-40a2-4823-b916-40add168310c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    36.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   36.00 MiB, K (f16):   18.00 MiB, V (f16):   18.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     7.88 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "\n",
      "llama_print_timings:        load time =     226.18 ms\n",
      "llama_print_timings:      sample time =      40.73 ms /    43 runs   (    0.95 ms per token,  1055.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     413.30 ms /    15 tokens (   27.55 ms per token,    36.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5657.09 ms /    42 runs   (  134.69 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:       total time =    6520.17 ms /    57 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I am a large language model, trained by Google. I am a conversational AI that can assist with a wide range of tasks, such as answering questions, writing different forms of text, and engaging in conversation.'\n"
     ]
    }
   ],
   "source": [
    "llm = GemmaChatLocal(model_name=model_name, hf_access_token=\"\")\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message1 = HumanMessage(content=\"Hi! Who are you?\")\n",
    "answer1 = llm.invoke([message1], max_tokens=60)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a3dea9-35e4-481d-95ac-7c3e4e444ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "assistant_role_name = \"Python Programmer\"\n",
    "user_role_name = \"Stock Trader\"\n",
    "task = \"Develop a trading bot for the stock market\"\n",
    "word_limit = 50  # word limit for task brainstorming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e14d6ae-ce59-4289-9c5f-fbb96a3812e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m task_specify_agent \u001b[38;5;241m=\u001b[39m CAMELAgent(task_specifier_sys_msg, ChatOpenAI)\u001b[38;5;66;03m#(temperature=1.0))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m task_specifier_msg \u001b[38;5;241m=\u001b[39m task_specifier_template\u001b[38;5;241m.\u001b[39mformat_messages(\n\u001b[1;32m     10\u001b[0m     assistant_role_name\u001b[38;5;241m=\u001b[39massistant_role_name,\n\u001b[1;32m     11\u001b[0m     user_role_name\u001b[38;5;241m=\u001b[39muser_role_name,\n\u001b[1;32m     12\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m     13\u001b[0m     word_limit\u001b[38;5;241m=\u001b[39mword_limit,\n\u001b[1;32m     14\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m specified_task_msg \u001b[38;5;241m=\u001b[39m \u001b[43mtask_specify_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_specifier_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecified task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecified_task_msg\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m specified_task \u001b[38;5;241m=\u001b[39m specified_task_msg\u001b[38;5;241m.\u001b[39mcontent\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mCAMELAgent.step\u001b[0;34m(self, input_message)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     30\u001b[0m     input_message: HumanMessage,\n\u001b[1;32m     31\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m     32\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_messages(input_message)\n\u001b[0;32m---> 34\u001b[0m     output_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_messages(output_message)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_message\n",
      "File \u001b[0;32m~/miniconda3/envs/t2v/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/t2v/lib/python3.10/site-packages/langchain_core/language_models/llms.py:985\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    992\u001b[0m         [prompt],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1001\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "task_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\n",
    "task_specifier_prompt = \"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\n",
    "Please make it more specific. Be creative and imaginative.\n",
    "Please reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\n",
    "task_specifier_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=task_specifier_prompt\n",
    ")\n",
    "task_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatOpenAI)#(temperature=1.0))\n",
    "task_specifier_msg = task_specifier_template.format_messages(\n",
    "    assistant_role_name=assistant_role_name,\n",
    "    user_role_name=user_role_name,\n",
    "    task=task,\n",
    "    word_limit=word_limit,\n",
    ")[0]\n",
    "specified_task_msg = task_specify_agent.step(task_specifier_msg)\n",
    "print(f\"Specified task: {specified_task_msg.content}\")\n",
    "specified_task = specified_task_msg.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f0ab7-511b-45c2-9c46-8bdef50780ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model_name = \"gemma:2b\"\n",
    "# model_name = \"llama2\"\n",
    "\n",
    "llm = Ollama(model=model_name)\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")\n",
    "query = \"Tell me a joke\"\n",
    "\n",
    "for chunks in llm.stream(query):\n",
    "    print(chunks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e2f5ee-6cc0-4c62-a91b-ffb2810ad832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SerperDevTool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model_name = \"llama2\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m----> 7\u001b[0m search_tool \u001b[38;5;241m=\u001b[39m \u001b[43mSerperDevTool\u001b[49m()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlocal_expert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      9\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m Agent(\n\u001b[1;32m     10\u001b[0m       role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Best Financial Analyst\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m       goal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mImpress all customers with your financial data \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m       ]\n\u001b[1;32m     25\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SerperDevTool' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model_name = \"gemma:2b\"\n",
    "# model_name = \"llama2\"\n",
    "\n",
    "llm = Ollama(model=model_name)\n",
    "search_tool = SerperDevTool()\n",
    "def local_expert(self):\n",
    "\treturn Agent(\n",
    "      role='The Best Financial Analyst',\n",
    "      goal=\"\"\"Impress all customers with your financial data \n",
    "      and market trends analysis\"\"\",\n",
    "      backstory=\"\"\"The most seasoned financial analyst with \n",
    "      lots of expertise in stock market analysis and investment\n",
    "      strategies that is working for a super important customer.\"\"\",\n",
    "      verbose=True,\n",
    "      llm=llm, # <----- passing our llm reference here\n",
    "      tools=[\n",
    "        BrowserTools.scrape_and_summarize_website,\n",
    "        SearchTools.search_internet,\n",
    "        CalculatorTools.calculate,\n",
    "        SECTools.search_10q,\n",
    "        SECTools.search_10k\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4dd9af-f679-48af-8dc4-ffa6f4aa9657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model_name = \"gemma:2b\"\n",
    "model_name = \"llama2\"\n",
    "ollama_openhermes = Ollama(model=model_name)\n",
    "# ollama_openhermes.invoke(\"Tell me a joke\")\n",
    "\n",
    "# search_tool = SerperDevTool()\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "ollama_openhermes = OllamaEmbeddings(model=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee30ed0-1c80-4502-bd4d-a434bf406b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = ollama_openhermes.embed_documents\n",
    "# query_result([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96117bd-cc30-4641-b2ab-f915ccab2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "llm = ChatOllama(model=\"llama2\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/expression_language/why\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
